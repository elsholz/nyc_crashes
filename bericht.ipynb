{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Analytics: NYC Crashes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einleitung\n",
    "\n",
    "In den Straßen New York Citys strotzt es gerade so vor Verkehr, wodurch Verkehrsunfälle an der Tagesordnung stehen. Durch das NYC Open Data Project werden die erfassten Verkehrsunfälle für die Allgemeinheit zugänglich.\n",
    "\n",
    "Thema des Berichts und unseres Projekts ist die Nutzung dieser öffentlich zugänglichen Unfalldaten zur Analyse und visuellen Auswertung. \n",
    "\n",
    "### Aufbau der Data Pipeline\n",
    "\n",
    "Für das Projekt soll eine Data-Pipeline aufgebaut werden, die die Daten in die Datenbank lädt. \n",
    "Von dort aus sollen die Daten von einem Python-Script zur Analyse angefragt werden.\n",
    "\n",
    "Beteiligt an der Data-Pipeline ist eine Kafka-Instanz, wobei ein Python-Script einen Producer dafür darstellt und ein weiteres Python-Script einen Consumer. Das Producer-Script lädt die Daten zeilenweise aus der Datenquelle, einer CSV-Datei, in Kafka ein, während der Consumer die Daten aus Kafka ausliest und in die MongoDB schreibt. Das Analyse-Script bezieht die Daten wiederum aus der MongoDB.\n",
    "\n",
    "#### Installation der benötigten Docker-Container\n",
    "\n",
    "Das Script `start_docker.sh` startet den MongoDB Docker Container, beziehungsweise erstellt bei Bedarf einen neuen Docker Container. Dabei wird, falls noch kein Image für die DockerDB vorhanden ist, das neueste Heruntergeladen. \n",
    "\n",
    "Die weiteren benötigten Docker Container sind in der Docker-Compose-Konfiguration (`docker-compose.yml`) vorhanden.\n",
    "Diese können mit `docker-compose up` erstellt oder heruntergeladen werden. \n",
    "Mit `docker-compose down` können die gestarteten Container gestoppt werden, wobei auf die Vollendigung des Befehls gewartet werden sollte.\n",
    "\n",
    "Auf manchen Systemen befindet sich die Docker Engine nach stoppen der Docker Container mit `docker-compose down` in einem nicht-revertierbarem Fehlerzustand, wobei ein _Neustart des Computers_ unausweichlich ist. Weitere Informationen zu dem Fehler, wobei jedoch _keine der angegebenen Workarounds genutzt werden sollte_, findet sich [auf GitHub](https://github.com/docker/for-linux/issues/162). \n",
    "\n",
    "### Datenquelle\n",
    "Als Datenquellen dienen die vom NYC Open Data Project bereitgestellten Unfalldaten des NYPD. Die detaillierten Unfalldaten befinden sich in 3 unterschiedlichen Datenquellen die unter Anderem als CSV-Datei verfügbar sind: [Motor Vehicle Collisions](https://data.cityofnewyork.us/browse?Data-Collection_Data-Collection=Motor+Vehicle+Collisions&q=crashes).\n",
    "\n",
    "Enthalten in den Daten sind die am Unfall beteiligten Verkehrsteilnehmer, die Verletzten und Toten, sowie die genaue Position des Unfalls. Darüberhinaus sind noch weitere Daten in den Quellen enthalten, auf die teilweise im Analyseabschnitt genauer eingegangen wird.\n",
    "\n",
    "### Analyseziele\n",
    "Ziel der Analyse der Daten ist herauszufinden, welche saisonalen und lokalen Zusammenhänge festzustellen sind, wodurch sich besondere Hotspots im Stadtraum New Yorks lokalisieren lassen. Des weiteren soll ermittelt werden, welche Verkehrsteilnehmer besonders oft getötet oder verletzt werden, sowie an welchen Stellen sich gerade schwerwiegende Unfälle häufen. Anhand einer animierten Karte kann dadurch das Unfallgeschehen über die Zeit betrachtet werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation der Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install kafka-python pymongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importieren benötigter Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "from pymongo import MongoClient\n",
    "import datetime as dt\n",
    "import requests\n",
    "import os\n",
    "from pathlib import Path\n",
    "from multiprocessing import Process\n",
    "from time import sleep\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Herunterladen der Datensätze\n",
    "Zum herunterladen der Datensätze werden die Daten von der Datenquelle mithilfe des nachfolgenden Python-Scripts heruntergeladen. Resultat sind drei `.csv`-Dateien die die Unfalldaten zeilenweise enthalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data/'):\n",
    "    os.mkdir('data')\n",
    "\n",
    "for file_name, download_url in [\n",
    "    ('crashes.csv', 'https://data.cityofnewyork.us/api/views/h9gi-nx95/rows.csv?accessType=DOWNLOAD'),\n",
    "    ('vehicles.csv', 'https://data.cityofnewyork.us/api/views/bm4k-52h4/rows.csv?accessType=DOWNLOAD'),\n",
    "    ('persons.csv', 'https://data.cityofnewyork.us/api/views/f55k-p6yu/rows.csv?accessType=DOWNLOAD'),\n",
    "]:\n",
    "    if not os.path.isfile(fp:= (Path('data') / file_name)):\n",
    "        with open(fp, 'wb') as crash_file:\n",
    "            crash_file.write(requests.get(download_url).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einlesen der Daten und senden über den Producer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier wird der Producer erstellt, sodass dieser verwendet werden kann, um Daten zu senden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = KafkaProducer(bootstrap_servers=['localhost:9092'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einlesen der CSV-Daten in Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Achtung: Viele Datensätze, dauert einige Minuten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_kafka(topic):\n",
    "    print(f'Started {topic} Producer process.')    \n",
    "    \n",
    "    with open((fn:=Path('data') / f'{topic}.csv')) as source:\n",
    "        for i, row in enumerate(source.readlines()):\n",
    "            if i:\n",
    "                producer.send(f'nyc_{topic}', value=bytearray(row, encoding='utf-8'), key=bytearray(str(i), encoding='utf-8'))\n",
    "            if not i % 25000 and i:\n",
    "                print(f'read {i} lines from {fn}.')\n",
    "                \n",
    "producers = {\n",
    "    'crashes': Process(target=send_to_kafka, args=['crashes']),\n",
    "    'vehicles': Process(target=send_to_kafka, args=['vehicles']),\n",
    "    'persons': Process(target=send_to_kafka, args=['persons']),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesen der Daten mit dem Consumer und Import in die Datenbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(\"mongodb://localhost:27017\")\n",
    "\n",
    "crashes_db = client['nyc_crashes']['crashes']\n",
    "persons_db = client['nyc_crashes']['persons']\n",
    "vehicles_db = client['nyc_crashes']['vehicles']\n",
    "\n",
    "deletion_processes = []\n",
    "\n",
    "try:\n",
    "    p = Process(taget=crashes_db.delete_many)\n",
    "    p.start()\n",
    "    deletion_processes.append(p)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    p = Process(taget=persons_db.delete_many)\n",
    "    p.start()\n",
    "    deletion_processes.append(p)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    p = Process(taget=vehicles_db.delete_many)\n",
    "    p.start()\n",
    "    deletion_processes.append(p)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "for p in deletion_processes:\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst werden die Daten aus den Topics `nyc_persons` und `nyc_vehicles` verarbeitet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_topic(topic, types):\n",
    "    consumer = KafkaConsumer(f'nyc_{topic}', bootstrap_servers=['localhost:9092'], auto_offset_reset=\"earliest\")\n",
    "    counter = 0\n",
    "    \n",
    "    print(f'Started {topic} Consumer process.')\n",
    "    \n",
    "    for row in consumer:\n",
    "        row = row.value.decode('utf-8').split(',')\n",
    "        print(row)\n",
    "        counter += 1\n",
    "        if counter and not counter % 100:\n",
    "            print(f'Read {counter} lines from Kafka topic {topic}')\n",
    "        res = {}\n",
    "        for idx, (db_field, field_type) in enumerate(types.items()):\n",
    "            try:\n",
    "                if row_data := row[idx]:\n",
    "                    res[db_field] = field_type.__call__(row_data) if not isinstance(row_data, field_type) else row_data\n",
    "                else:\n",
    "                    res[db_field] = None   \n",
    "            except Exception as e:\n",
    "                res[db_field] = None\n",
    "\n",
    "        # make sure the document is identifiable\n",
    "        if res['_id']:\n",
    "            db_by_topic[topic].insert_one(res)\n",
    "\n",
    "            \n",
    "db_by_topic = {\n",
    "    'vehicles': vehicles_db,\n",
    "    'persons': persons_db,\n",
    "    'crashes': crashes_db\n",
    "}\n",
    "            \n",
    "consumers = {\n",
    "    'vehicles': Process(target=process_topic, args=(\n",
    "        'vehicles',  {\n",
    "            'unique_id': str,\n",
    "            '_id': str,\n",
    "            'crash_date': str,\n",
    "            'crash_time': str,\n",
    "            'vehicle_id': str,\n",
    "            'state_registration': str,\n",
    "            'vehicle_type': str,\n",
    "            'vehicle_make': str,\n",
    "            'vehicle_model': str,\n",
    "            'vehicle_year': str,\n",
    "            'travel_direction': str,\n",
    "            'vehicle_occupants': str,\n",
    "            'driver_sex': str,\n",
    "            'driver_license_status': str,\n",
    "            'driver_license_jurisdiction': str,\n",
    "            'pre_crash': str,\n",
    "            'point_of_impact': str,\n",
    "            'vehicle_damage': str,\n",
    "            'vehicle_damage_1': str,\n",
    "            'vehicle_damage_2': str,\n",
    "            'vehicle_damage_3': str,\n",
    "            'public_property_damage': str,\n",
    "            'public_property_damage_type': str,\n",
    "            'contributing_factor_1': str,\n",
    "            'contributing_factor_2': str\n",
    "        }\n",
    "    )),\n",
    "    'persons': Process(target=process_topic, args=(\n",
    "        'persons', {\n",
    "            'unique_id': str,\n",
    "            '_id': str,\n",
    "            'crash_date': str,\n",
    "            'crash_time': str,\n",
    "            'person_id': str,\n",
    "            'person_type': str,\n",
    "            'person_injury': str,\n",
    "            'vehicle_id': str,\n",
    "            'person_age': str,\n",
    "            'ejection': str,\n",
    "            'emotional_status': str,\n",
    "            'bodily_injury': str,\n",
    "            'position_in_vehicle': str,\n",
    "            'safety_equipment': str,\n",
    "            'ped_location': str,\n",
    "            'ped_action': str,\n",
    "            'complaint': str,\n",
    "            'ped_role': str,\n",
    "            'contributing_factor_1': str,\n",
    "            'contributing_factor_2': str,\n",
    "            'person_sex': str\n",
    "        }\n",
    "    ))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starten der Prozesse\n",
    "Nun werden die Prozesse zum Produzieren und Konsumieren der Daten aus `vehicles.csv` und `persons.csv` gestartet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in 'persons vehicles'.split(' '):\n",
    "    producers[topic].start()\n",
    "\n",
    "sleep(10)\n",
    "    \n",
    "for topic in 'persons vehicles'.split(' '):\n",
    "    consumers[topic].start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just in case you need to stop processes:\n",
    "for topic in 'persons vehicles'.split(' '):\n",
    "    consumers[topic].terminate()\n",
    "    producers[topic].terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Danach können diese Daten mit den Daten aus `nyc_crashes` gejoined werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_crashes():\n",
    "    consumer = KafkaConsumer('nyc_crashes', bootstrap_servers=['localhost:9092'], auto_offset_reset=\"earliest\")\n",
    "    \n",
    "    types = {\n",
    "                'crash_date': str, \n",
    "                'crash_time': str, \n",
    "                'borough': str, \n",
    "                'zip_code': int, \n",
    "                'latitude': float, \n",
    "                'longitude': float, \n",
    "                'location': str, \n",
    "                'on_street_name': str, \n",
    "                'cross_street_name': str, \n",
    "                'off_street_name': str, \n",
    "                'persons_injured': int, \n",
    "                'persons_killed': int, \n",
    "                'pedestrians_injured': int, \n",
    "                'pedestrians_killed': int, \n",
    "                'cyclists_injured': int, \n",
    "                'cyclists_killed': int, \n",
    "                'motorists_injured': int, \n",
    "                'motorists_killed': int, \n",
    "                'contributing_factor_vehicle_1': str,\n",
    "                'contributing_factor_vehicle_2': str,\n",
    "                'contributing_factor_vehicle_3': str,\n",
    "                'contributing_factor_vehicle_4': str,\n",
    "                'contributing_factor_vehicle_5': str,\n",
    "                '_id': int,\n",
    "                'vehicle_type_code_1': str,\n",
    "                'vehicle_type_code_2': str,\n",
    "                'vehicle_type_code_3': str,\n",
    "                'vehicle_type_code_4': str,\n",
    "                'vehicle_type_code_5': str,\n",
    "    }\n",
    "    \n",
    "    for row in consumer: \n",
    "        row = row.value.decode('utf-8').split(',')\n",
    "        counter += 1\n",
    "        if counter and not counter % 25000:\n",
    "            print(counter)\n",
    "        res = {}\n",
    "        for idx, (db_field, field_type) in enumerate(types.items()):\n",
    "            try:\n",
    "                if row_data := row[idx]:\n",
    "                    res[db_field] = field_type.__call__(row_data) if not isinstance(row_data, field_type) else row_data\n",
    "                else:\n",
    "                    res[db_field] = None\n",
    "            except Exception as e:\n",
    "                res[db_field] = None\n",
    "\n",
    "        # make sure the document is identifiable\n",
    "        if res['_id']:\n",
    "            res['vehicles'] = list(db_by_topic['vehicles'].find({'_id': {'$eq': res['_id']}}))\n",
    "            res['persons'] = list(db_by_topic['persons'].find({'_id': {'$eq': res['_id']}}))\n",
    "\n",
    "            db_by_topic['crashes'].insert_one(res)\n",
    "\n",
    "consumers['crashes'] = Process(target=process_crashes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starten des Prozesses\n",
    "Im letzten Schritt müssen die beteiligten Prozesse gestartet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "producers['crashes'].start()\n",
    "consumers['crashes'].start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers=['localhost:9092'])\n",
    "\n",
    "for i in range(1, 13):\n",
    "    if i < 10:\n",
    "        i = \"0%s\" % i\n",
    "    dataset = open('./data/2019/yellow-cabs-2019-%s.csv' % i, encoding='utf-8')\n",
    "    rows = dataset.readlines()[1:]\n",
    "\n",
    "    for i, row in enumerate(rows):\n",
    "        producer.send('yellow-cabs', value=bytearray(row, encoding='utf-8'), key=bytearray(str(i), encoding='utf-8'))\n",
    "\n",
    "for i in range(1, 7):\n",
    "    if i < 10:\n",
    "        i = \"0%s\" % i\n",
    "    dataset = open('./data/2019/yellow-cabs-2020-%s.csv' % i, encoding='utf-8')\n",
    "    rows = dataset.readlines()[1:]\n",
    "\n",
    "    for i, row in enumerate(rows):\n",
    "        producer.send('yellow-cabs', value=bytearray(row, encoding='utf-8'), key=bytearray(str(i), encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "consumer = KafkaConsumer('yellow-cabs', bootstrap_servers=[\"\"])\n",
    "\n",
    "client = MongoClient(\"\")\n",
    "\n",
    "yellow_collection = client['datawarehouse']['bg-yellowcabs']\n",
    "yellow_collection.delete_many({})\n",
    "\n",
    "count = 0\n",
    "\n",
    "for msg in consumer: \n",
    "    count += 1\n",
    "    print('Received new message: %s' % count)\n",
    "    values = msg.value.decode('utf-8').split(',')\n",
    "    \n",
    "    yellow_collection.insert_one({\n",
    "        'pickup_datetime': dt.datetime.strptime(values[1], \"%Y-%m-%d %H:%M:%S\"),\n",
    "        'dropoff_datetime': dt.datetime.strptime(values[2], \"%Y-%m-%d %H:%M:%S\"),\n",
    "        'passenger_count': int(values[3]),\n",
    "        'trip_distance': float(values[4]),\n",
    "        'PULocationID': values[5],\n",
    "        'DOLocationID': values[6],\n",
    "        'payment_type': int(values[9]),\n",
    "        'fare_amount': float(values[10]),\n",
    "        'tip_amount': float(values[15]),\n",
    "        'total_amount': float(values[16])\n",
    "    })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (data_analytics)",
   "language": "python",
   "name": "pycharm-caae33a3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
