{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einleitung\n",
    "\n",
    "In den Straßen New York Citys herrscht ein hohes Verkehrsaufkommen, wodurch Verkehrsunfälle an der Tagesordnung sind. Durch [NYC Open Data](https://opendata.cityofnewyork.us/) werden die erfassten Verkehrsunfälle für die Allgemeinheit zugänglich gemacht.\n",
    "\n",
    "Thema dieses Berichtes und Thema unseres Projektes ist es, die öffentlich zugänglichen Unfalldaten zu nutzen, um darauf die im Abschnitt `Analyseziele` beschriebenen Analysen durchzuführen und die Ergebnisse zu visualisieren.\n",
    "\n",
    "## Ausführungsplan\n",
    "\n",
    "Bevor der eigentliche Bericht und der entsprechende Code genauer behandelt werden, wird hier erläutert, was installiert und beachtet werden muss, um einen möglichst reibungslosen Ablauf zu garantieren.\n",
    "\n",
    "### Installation der benötigten Docker-Container\n",
    "\n",
    "Damit keine weiteren Komponenten manuell installiert werden müssen und keine Änderungen am ausführenden System entstehen, werden alle Komponenten durch [Docker](https://docs.docker.com/get-docker/) installiert und ausgeführt. Somit wird Docker benötigt und muss installiert werden.\n",
    "\n",
    "Die benötigten Docker-Container sind in der Docker-Compose-Konfiguration (`docker-compose.yml`) definiert.\n",
    "Diese können mit `docker-compose up -d` heruntergeladen und gestartet werden. \n",
    "Mit `docker-compose down` können die gestarteten Container gestoppt werden. Das Terminal sollte nicht geschlossen werden, bevor alle Container durch `docker-compose down` gestoppt vollständig gestoppt wurden.\n",
    "\n",
    "Durch Nutzung einer angepassten `docker-compose.yml` konnten Fehler, die mit der Verwendung einer vorgefertigten `docker-compose.yml` entstanden sind, ausgeräumt werden.\n",
    "\n",
    "### Starten des Programmcodes\n",
    "\n",
    "Nachdem alle Container über `docker-compose up -d` gestartet wurden, kann mit der Ausführung des Jupyter Notebooks begonnen werden. Hierbei kann unter `Kernel` die Option `Restart & Run All` genutzt werden, um das komplette Notebook auszuführen. Sollte der Code nicht auf einem Windows-System ausgeführt werden, so befindet sich im Kapitel `Starten des Join-Prozesses (außer auf Windows)` eine Art Breakpoint. Dieser Breakpoint dient (wie unten beschrieben) dazu, dass der Join-Prozess nicht ausgeführt wird, bevor alle notwendigen Daten geladen sind. Zur Ausführung des Codes unter dem Breakpoint sind die Anweisungen am Anfang des entsprechenden Kapitels zu befolgen.\n",
    "\n",
    "## Aufbau der Data Pipeline\n",
    "\n",
    "Für das Projekt soll eine Data-Pipeline aufgebaut werden, welche die Daten in die Datenbank lädt. \n",
    "Von dort aus sollen die Daten von einem Python-Script zur Analyse angefragt werden.\n",
    "\n",
    "Beteiligt an der Data-Pipeline ist eine Kafka-Instanz, wobei ein Python-Script einen Producer dafür darstellt und ein weiteres Python-Script einen Consumer. Das Producer-Script lädt die Daten zeilenweise aus der Datenquelle, einer CSV-Datei, in Kafka ein, während der Consumer die Daten aus Kafka ausliest und in die MongoDB schreibt. Das Analyse-Script bezieht die Daten wiederum aus der MongoDB.\n",
    "\n",
    "## Datenquellen\n",
    "\n",
    "Als Datenquellen dienen die vom NYC Open Data Project bereitgestellten Unfalldaten des NYPD. Die detaillierten Unfalldaten befinden sich in drei unterschiedlichen Datenquellen die unter Anderem als CSV-Datei verfügbar sind: [Motor Vehicle Collisions](https://data.cityofnewyork.us/browse?Data-Collection_Data-Collection=Motor+Vehicle+Collisions&q=crashes).\n",
    "\n",
    "Enthalten in den Daten sind die am Unfall beteiligten Verkehrsteilnehmer, die Verletzten und Toten, sowie die genaue Position des Unfalls. Darüberhinaus sind noch weitere Daten in den Quellen enthalten, auf die teilweise im Analyseabschnitt genauer eingegangen wird.\n",
    "\n",
    "## Analyseziele\n",
    "\n",
    "Nach der Verarbeitung und Aufbereitung der Daten soll aus der Analyse hervorgehen, welche saisonalen und lokalen Zusammenhänge aus den Unfalldaten gelesen werden können. Des Weiteren soll das Ergebnis eine Aussage darüber geben, ob sich bestimmte Unfallhotspots innerhalb von New York lokalisieren lassen und ob diese Hotspots sich mit der Zeit ändern. Während der Ansicht der Unfallhotspots soll es auch möglich sein, zu erkennen, an welchem Hotspot sich besonders viele schwerwiegende Unfälle ereignen (Unfälle mit vielen verletzten oder getöteten Verkehrsteilnehmern). Zuletzt soll aus der Analyse hervorgehen, welche Verkehrsteilnehmer besonders oft getötet und verletzt werden. Hierzu zählen sowohl die Füßgänger, die Fahrradfahrer als auch die Fahrer von motorisierten Fahrzeugen.\n",
    "\n",
    "Die Visualisierung der Daten erfolgt über verschiedene Diagramme sowie eine Heatmap, welche mehrere Filtermöglichkeiten bereitstellt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation der Python Packages\n",
    "\n",
    "Damit nicht alles vollständig von uns entwickelt werden muss, wurden verschiedene Bibliotheken verwendet. Auf diese Weise kann sich die Umsetzung auf das Wesentliche beziehen.\n",
    "\n",
    "[Apache Kafka](https://kafka.apache.org/) soll dazu genutzt werden, mit Hilfe eines Producers und eines Consumers die Datensätze zeilenweise einzulesen. Dies wird durch die [kafka-python](https://github.com/dpkp/kafka-python) Bibliothek stark vereinfacht.\n",
    "\n",
    "Während der Kafka Consumer die Datensätze registiert und ausliest müssen diese ein die MongoDB eingetragen werden. Um dieses Verfahren zu vereinfachen, wird die [pymongo](https://pypi.org/project/pymongo/) Bibliothek genutzt.\n",
    "\n",
    "Nachdem alle Daten in der Datenbank sind, folgt die Auswertung der Daten. Um aus den Daten eine visuelle Darstellung in Form einer Heatmap aufbauen zu können, kommt die [folium](https://github.com/python-visualization/folium) Bibliothek zum Einsatz.\n",
    "\n",
    "Es folgt die Installation der genannten Bibliotheken über `pip install`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install kafka-python pymongo folium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importieren benötigter Module\n",
    "\n",
    "Damit die verschiedenen Funktionen und Datentypen der Bibliotheken genutzt werden können, müssen die entsprechenden Module importiert werden. Im folgenden Abschnitt werden einige der notwendigen Module importiert.\n",
    "\n",
    "Zusätzlich hat sich während der Entwicklung gezeigt, dass je nach Betriebssystem Unterschiede bezüglich der Paralellisierung von Prozessen entstehen. Aus diesem Grund wird hier eine Variable `windows` deklariert, mit der während der Ausführung bestimmter Funktionen zwischen Windows und anderen Betriebssystemen unterschieden werden kann.\n",
    "\n",
    "Bei der `DEBUG` Variable handelt es sich um eine Variable, welche beim Wert `TRUE` dafür sorgt, dass nur ein Bruchteil der Daten aus den CSV-Dateien importiert wird. Der Import aller Daten (`DEBUG=FALSE`) kann mehrere Stunden in Anspruch nehmen, da es über 9 Millionen Zeilen sind (Stand 12.11.2020). `Mit DEBUG=TRUE` Variable dauert der Import der Daten nur wenige Minuten.\n",
    "\n",
    "Zuletzt wird hierbei die Breite des Jupyter Notebooks auf 100% gestellt, um die Lesbarkeit zu verbessern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "from pymongo import MongoClient\n",
    "import datetime as dt\n",
    "import requests\n",
    "import os\n",
    "from pathlib import Path\n",
    "from multiprocessing import Process\n",
    "from time import sleep\n",
    "from data.schemas import schemas\n",
    "import platform\n",
    "import folium\n",
    "windows = True if 'Windows' in platform.system() else False \n",
    "DEBUG = True\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\n",
    "    \"<style>.container { width:100% !important; }</style>\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verarbeitung der Daten\n",
    "\n",
    "Um mit der Verarbeitung der Daten zu beginnen, müssen die Daten zunächst aufbereitet und in die MongoDB eingefügt werden. Damit mit Hilfe der MongoDB verschiedene Analysen durchgeführt werden können, sind verschiedene Schritte notwendig, welche in diesem Kapitel nacheinander erläutert und durchgeführt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Herunterladen der Datensätze\n",
    "\n",
    "Da die Daten nur verarbeitet werden können, während sie diesem Notebook zur Verfügung stehen, müssen die Daten im ersten Schritt heruntergeladen werden. Hierzu werden die Daten von der Datenquelle mithilfe des nachfolgenden Python-Scripts heruntergeladen. Resultat sind drei `.csv`-Dateien, welche die Unfalldaten zeilenweise enthalten.\n",
    "Die Dateien werden im Ordner `data` gespeichert, damit sie nur einmalig heruntergeladen werden müssen. Der Ordner befindet sich im gleichen Verzeichnis wie dieses Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data/'):\n",
    "    os.mkdir('data')\n",
    "\n",
    "for file_name, download_url in [\n",
    "    ('crashes.csv', 'https://data.cityofnewyork.us/api/'\n",
    "     'views/h9gi-nx95/rows.csv?accessType=DOWNLOAD'),\n",
    "    ('vehicles.csv', 'https://data.cityofnewyork.us/api/'\n",
    "     'views/bm4k-52h4/rows.csv?accessType=DOWNLOAD'),\n",
    "    ('persons.csv', 'https://data.cityofnewyork.us/api/'\n",
    "     'views/f55k-p6yu/rows.csv?accessType=DOWNLOAD'),\n",
    "]:\n",
    "    if not os.path.isfile(fp:= (Path('data') / file_name)):\n",
    "        with open(fp, 'wb') as crash_file:\n",
    "            crash_file.write(requests.get(download_url).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produzieren der CSV-Daten in Kafka\n",
    "Nachdem die Daten heruntergeladen wurden, beginnt die eigentliche Verarbeitung.\n",
    "Im folgenden Codeabschnitt wird hierfür die Funktion `send_to_kafka` genutzt, die in das Modul `pipeline_tools.py` ausgelagert wurde. \n",
    "Die Funktion erstellt zunächst eine Instanz des `KafkaProducer`, entsprechend des angegebenen Topics.\n",
    "Um die Daten zu produzieren muss zeilenweise über die entsprechende `.csv` Datei iteriert werden, wobei jedoch ein Laden der gesamten Datei in den RAM vermieden wird, da es sich um einen Generator handelt.\n",
    "Nachdem eine Zeile ausgelesen wurde, wird diese in das entsprechende `topic` gesandt und muss von einem `KafkaConsumer` wieder ausgelesen werden, um die Daten nutzen zu können.\n",
    "\n",
    "Für jedes der 3 Topics wird ein eigener Producer Process definiert.\n",
    "Die einzelnen Prozesse können auf UNIX-basierten Systemen parallel ausgeführt werden, wodurch die Ausführung durch Nutzung von mehr logischen Prozessoren beschleunigt wird. \n",
    "Hierbei werden gezielt Prozesse gestartet und keine Threads, da Multithreading in Python mit geteilten Variablen in der Regel mit Race Conditions verbunden ist und daher zu unbrauchbaren Ergebnissen führt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline_tools import send_to_kafka\n",
    "                \n",
    "producers = {\n",
    "    'crashes': Process(\n",
    "        target=send_to_kafka, args=[\n",
    "            'crashes', 15000 if DEBUG else None\n",
    "    ]),\n",
    "    'vehicles': Process(\n",
    "        target=send_to_kafka, args=[\n",
    "            'vehicles', 5000 if DEBUG else None\n",
    "    ]),\n",
    "    'persons': Process(\n",
    "        target=send_to_kafka, args=[\n",
    "            'persons', 5000 if DEBUG else None\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Konsumieren der produzierten Daten und Import in die Datenbank\n",
    "\n",
    "Im nächsten Schritt wird die Funktion aufgerufen, mit der die Daten aus Kafka ausgelesen und in die MongoDB eingefügt werden. Diese Funktion befindet sich ebenfalls im Modul `pipeline_tools.py`.\n",
    "\n",
    "In der Funktion wird zunächst eine Verbindung zur MongoDB aufgebaut.\n",
    "Sobald die Verbindung zur Datenbank bereit ist, wird ein `KafkaConsumer` erstellt, der das übergebene `topic` abonniert. \n",
    "Durch das Abonnieren eines Topics erhält der Consumer zeilenweise die zuvor in das Topic gelesenen Daten. \n",
    "Jede Zeile wird nach dem Einlesen zunächst in eine Liste konvertiert, indem ein CSV-Reader darauf angewandt wird. \n",
    "Da es sich um Daten im CSV-Format handelt, die Strings mit KOmmas enthalten, würde ein Aufruf von `split(',')` fehlerhafte Ergebnisse liefern.\n",
    "\n",
    "Durch das Trennen in die einzelnen Bestandteile, sind die Daten nun bereit als Key-Value Paare in das entsprechende Dokument innerhalb der MongoDB persistiert zu werden.\n",
    "Dabei wird das mitgegebene Schema verwendet um die Datentypen bei Bedarf zu konvertieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline_tools import process_topic\n",
    "\n",
    "consumers = {\n",
    "    'vehicles': Process(\n",
    "        target=process_topic, args=(\n",
    "        'vehicles', schemas['vehicles']\n",
    "    )),\n",
    "    'persons': Process(\n",
    "        target=process_topic, args=(\n",
    "        'persons', schemas['persons']\n",
    "    )),\n",
    "    'crashes': Process(\n",
    "        target=process_topic, args=(\n",
    "        'crashes', schemas['crashes']\n",
    "    )),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starten der Prozesse\n",
    "\n",
    "Nachdem die Funktionen, die für das Speichern der CSV-Daten in die Datenbank zuständig sind, nun fertig definiert sind, werden diese hier aufgerufen. Da die Daten zu Personen und Fahrzeugen (Topic `nyc_persons` und `nyc_vehicles`) im Anschluss an das Persistieren in der Datenbank mit den Unfällen (`crashes`) verbunden werden müssen, werden sie vollständig verarbeitet, bevor die Daten zu den Unfällen in die Datenbank geschrieben werden. \n",
    "Dies wurde für das Windows Betriebssystem und andere Betriebssysteme unterschiedlich umgesetzt:\n",
    "\n",
    "Bei der Ausführung der Funktionen wird zwischen Microsoft Windows und anderen Betriebssystemen unterschieden:\n",
    "\n",
    "**Windows:** Da es nicht möglich ist mehrere Child Processes im Jupyter Notebook zu starten, müssen die Funktionsaufrufe nacheinander und blockierend erfolgen. Das Problem hat sich darin geäußert, dass die Prozesse zwar ohne Fehlermeldung gestartet werden konnten, aber in keiner Form Output gezeigt haben, weder in der Datenbank, noch im Kafka-Topic oder dem Notebook selbst.\n",
    "Durch die Synchronität können direkt alle Funktionen aufgerufen werden. \n",
    "\n",
    "**UNIX:** Hier werden die Producer für die Topics `nyc_crashes`, `nyc_vehicles`, `nyc_persons` parallel gestartet. Außerdem werden die Consumer der Topics `nyc_vehicles` und `nyc_persons` kurz darauf gestartet. Sobald die beiden Consumer-Prozesse fertig sind, was daran erkenntlich wird, wenn sie nicht mehr in die Datenbank schreiben, können die nachfolgenden Prozesse manuell gestartet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline_tools import aggregate_data\n",
    "\n",
    "if windows:\n",
    "    limit = send_to_kafka('vehicles', 5000 if DEBUG else None)\n",
    "    process_topic('vehicles', schemas['vehicles'], limit)\n",
    "    limit = send_to_kafka('persons', 5000 if DEBUG else None)\n",
    "    process_topic('persons', schemas['persons'], limit)\n",
    "    limit = send_to_kafka('crashes', 15000 if DEBUG else None)\n",
    "    process_topic('crashes', schemas['crashes'], limit)\n",
    "    \n",
    "    aggregate_data()\n",
    "else:\n",
    "    for topic in 'persons vehicles crashes'.split(' '):\n",
    "        producers[topic].start()\n",
    "    sleep(2)\n",
    "    for topic in 'persons vehicles'.split(' '):\n",
    "        consumers[topic].start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starten des Join-Prozesses (UNIX)\n",
    "Im letzten Schritt muss der Prozess gestartet werden, der das Verbinden der beiden Tabellen `vehicles` und `persons` in die Daten des Topics `nyc_crashes` durchführt. Dieser Schritt wurde auf Windows bereits im vorherigen Codeabschnitt erledigt, da dieser auf Windows nicht paralellisiert wird. Das heißt, dass die Vollständigkeit der Daten bereits durch Synchronität gewährleistet wird.\n",
    "\n",
    "Für UNIX-Systeme ist wichtig, dass die parallelisierten Prozesse zum Produzieren aller Daten und zum Einlesen der `nyc_vehicles` und `nyc_persons` Topics beendet sind. Die Ausführung des vorherigen Codeblocks sollte also abgeschlossen sein. Damit die folgende Funktion bei einem `Kernel` → `Restart & Run All` nicht zu vor Beendigung der geforkten Prozesse ausgeführt wird, wurden 2 Breakpoints eingefügt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not windows:\n",
    "    assert False, 'non-Windows breakpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausführung erst nach Beendigung der drei Producer\n",
    "if not windows:\n",
    "    consumers['crashes'].start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not windows:\n",
    "    assert False, 'non-Windows breakpoint'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die letzte Collection die in der MongoDB erstellt wird beinhaltet aggregierte Daten.\n",
    "Die Funktion `aggregate_data`, die in das Modul `pipeline_tools.py` exportiert wurde, iteriert über die Daten aus der `crashes`-Collection und summiert für die einzelnen Arten von Verkehrsteilnehmern jeweils nach Jahr und nach Monat in einer geschachtelteten Collection die Toten, Verletzten und Zuschadengekommenen.\n",
    "\n",
    "Die erstellte Collection `crashes_by_year` erlaubt Zugruff auf die aggregierten Unfallzahlen auf Jahresbasis und auf Jahres-Monats-Basis. \n",
    "Eine weitere Verschachtelung nach Tagen innerhalb der Monate, oder nach Wochen innerhalb des Jahres währe außerdem denkbar gewesen.\n",
    "Da aber die Analysemöglichkeiten bereits mit monatlicher Staffelung mehr als ausreichend sind, und eine Aggregierung der Zahlen auf Tagesebene keine Anwendung mehr gefunden hätte, haben wir diese ausgelassen.\n",
    "\n",
    "Eine tagesbasierte Aggregation der Unfallzahlen wäre nur sinnvoll gewesen, wenn die Unfälle dadurch nachvollziehbar visualisiert worden wären, etwa in Form eines Zeitraffers auf einer animierten Karte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausführung erst nach Beendigung des `nyc_crashes` Consumers\n",
    "if not windows:\n",
    "    aggregate_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datenvisualisierung\n",
    "\n",
    "Da an dieser Stelle alle Daten vollständig in die Datenbank importiert und anschließend so aufbereitet wurden, dass sie für eine visuelle Analyse verwendet werden können, beginnt an dieser Stelle die Datenanalyse. Wie im Kapitel `Analyseziele` bereits erwähnt, werden zur visuellen Darstellung Diagramme und eine Heatmap genutzt, welche vor der Erstellung nochmal detailliert erläutert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from analysis_tools import (\n",
    "    monthly, monthly_sum, \n",
    "    show_graph_injury, show_graph_type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "with MongoClient('mongodb://localhost:27017/') as client:\n",
    "    database = client[\"nyc_crashes\"]\n",
    "    crashes = database.crashes\n",
    "    crashes_by_year = database.crashes_by_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color:red\">Hier noch irgendwo erklären, wie das Diagram erstellt wurde (technisch)</h4>\n",
    "\n",
    "Aus dem folgenden Diagram, welches sich mit der Art der Verletzung auseinandersetzt, können verschiedene Schlüsse gezogen werden.<br>\n",
    "Offensichtlich erkennbar ist beispielsweise:\n",
    "\n",
    "**1.** Die Anzahl an verletzten Personen ist bei einem Verkehrsunfall deutlich höher als die Anzahl der getöteten Personen\n",
    "\n",
    "**2.** In den kalten Jahreszeiten (Winter & Frühling) gibt es weniger verletzte und getötete Verkehrsteilnehmer. Vermutlich sind die Leute zu dieser Zeit weniger unterwegs. Eine Ausnahme ist hierbei der Dezember, was vermutlich an den Festtagen liegt\n",
    "\n",
    "Etwas nicht so offensichtliches, was jedoch auch aus dem Diagram hervor geht: Betrachtet man die Jahre 2012-2014 und die Jahre 2018-2020, so sieht man, dass die Letalität von Unfällen abgenommen hat, was dafür spricht, dass die Verkehrsteilnehmer entweder vorsichtiger fahren oder die Sicherheit von Fahrzeugen sich über die Jahre verbessert hat.\n",
    "\n",
    "## Interact\n",
    "Das `interact` ermöglicht es, ein Dropdown-Menü anzeigen zu können. Dafür benötigt es die Angabe einer Funktion und einer Liste von Daten, die im Dropdown zur Auswahl stehen sollen. Wenn eine der Optionen gewählt wird, wird die angegebene Funktion, im Falle des ersten Diagramms `monthly`, mit dem ausgewählten Wert ausgeführt. Dabei wird aus der Liste nur ein einziger Wert an die Funktion übergeben. \n",
    "\n",
    "## Daten aus MongoDB laden\n",
    "Für die Liste werden alle `_id` Elemente gesucht, die in unserem Datenmodell die Jahre darstellen. So hat der Nutzer die Möglichkeit, im Dropdown ein Jahr auszuwählen.\n",
    "\n",
    "Die aufgerufene Funktion hat die Aufgabe, die benötigten Daten aus der MongoDB zu laden und in Form zu bringen. Für die Darstellung werden Listen mit X- und Y-Werten benötigt.\n",
    "Zum Anzeigen der Daten wollten wir auf der X-Achse die Monate haben und auf der Y-Achse die Anzahl der Unfälle darstellen. Damit ist im Diagramm ein Jahresverlauf zu erkennen und die Werte sind gut mit einander vergleichbar.\n",
    "\n",
    "## Bar Chart\n",
    "Für die Darstellung griffen wir auf die Python Library `matplotlib` zurück. Diese stellt verschiedene Funktionen bereit, um Daten in verschiedenen Formen darzustellen. So können Diagramme, Scatterplots, Graphen und viele mehr damit dargestellt werden. Wir haben uns dazu entschieden, die Daten in Balkendiagrammen darzustellen.\n",
    "Mit matplotlib gibt es die Möglichkeit, mehrere Daten in einem Balken darzustellen. Dies haben wir uns zur Nutze gemacht und im ersten Diagramm zwei Balken nebeneinander. Damit kann die Anzahl der verletzten Personen bei Unfällen mit der Zahl der umgekommenen Personen bei einem Unfall direkt verglichen werden.\n",
    "Im zweiten Diagramm sieht man die Gesamtzahl der Unfälle, aufgeteilt danach, ob bei dem Unfall ein Fußgänger, Fahrradfahrer oder ein Kraftfahrzeug involviert war.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(monthly, Jahr=[data['_id'] for data in crashes_by_year.find().sort('_id', -1)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color:red\">Hier noch irgendwo erklären, wie das Diagram erstellt wurde (technisch)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(monthly_sum, Jahr=[data['_id'] for data in crashes_by_year.find().sort('_id', -1)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saisonale Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis_tools import show_line_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_line_chart(damage='injured', kinds=['pedestrians', 'motorists', 'cyclists'])\n",
    "show_line_chart(damage='injured', kinds=['pedestrians', 'cyclists'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trendvisualisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis_tools import show_stacked_victims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_stacked_victims()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geographische Visualisierung\n",
    "\n",
    "Mit der interaktiven Unfallkarte ist es möglich, die Daten nach verschiedenen Kriterien zu filtern und diese als Heatmap darzustellen. So können beispielsweise die Unfälle, bei denen Fußgänger verletzt wurden und die Unfälle bei denen Fußgänger getötet wurden einzeln oder gemeinsam betrachtet wurden.<br>\n",
    "Alle möglichen Filter können oben Rechts auf der Karte gefunden und entsprechend an- oder abgewählt werden. Des Weiteren ist es möglich die Karte heran- und heraus zu zoomen, um einen genaueren Blick auf die Unfälle an einer beliebigen Stelle in New York zu werfen.\n",
    "\n",
    "Die Unfallkarte wird mit Hile der Library [folium](https://github.com/python-visualization/folium) erstellt und benötigt zur Erstellung lediglich die Daten, die angezeigt werden sollen und ein wenig Konfiguration. Die Funktion `make_map()`, in der sich der Code für die Erstellung der Karte befindet wurde in das Modul `analysis_tools.py` ausgelagert.\n",
    "\n",
    "Die Funktion verbindet sich mit der Datenbank und list alle benötigten Daten aus. \n",
    "Da die Crash-Daten von der MongoDB in den RAM geladen werden, kann dies auf Computern mit weniger als 16 GB RAM etwas länger dauern. \n",
    "Die Funktion erstellt eine Basiskarte, die Initial auf die Koordinaten New York Citys gelegt wird.\n",
    "Zu der Karte werden in der Funktion verschiedene Heatmaps in Layern hinzugefügt, an denen die Toten-/ Verletzten-Dichte erkenntlich werden. \n",
    "Diese einzelnen Layer sind beschrieben und können in dem Layer Selector (oben rechts) ein- und ausgeblendet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis_tools import make_map\n",
    "crash_map = make_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fazit\n",
    "\n",
    "<h4 style=\"color:red\">Erklärung der Analyse der einzelnen Diagramme vielleicht direkt unter/über Diagramme und Karte packen</h4>\n",
    "\n",
    "Nachdem die Daten an dieser Stelle sowohl durch die oben gezeigten Diagramme, als auch durch die gezeigte Heatmap visualisiert wurde, können einige interessante Ergebnisse aus den dargestellten Statistiken gelesen werden.\n",
    "\n",
    "Das zweite Diagramm, welches die Anzahl der Unfälle nach Mobilitätstyp (zu Fuß, mit dem Fahrrad, mit Kraftfahrzeugen) unterteilt gibt ebenfalls eine Auskunft darüber, dass im Frühling und im Winter weniger Unfälle passieren. Zusätzlich sieht man, dass in den kalten Jahreszeiten weniger Leute mit dem Fahrrad verunfallen. Hieraus lässt sich schließen, dass die New Yorker Bevölkerung lieber mit dem Fahrrad fährt, wenn es warm ist.\n",
    "\n",
    "Sowohl im ersten, als auch im zweiten Diagram zeichnen sich deutlich die Auswirkungen der Coronakrise auf den Verkehr ab. In den Monaten, in denen das Coronavirus für eine Vielzahl an Lockdowns gesorgt hat (ab März), hat die Anzahl der Unfälle drastisch abgenommen. Dies spricht dafür, dass die Bürger von New York während des Lockdowns weniger unterwegs waren.\n",
    "\n",
    "Betrachtet man die Heatmap, so kann man anhand der verschiedenen Filter viele verschiedene Schlüsse ziehen.<br>\n",
    "Zunächst wird ersichtlich, dass in New York quasi überall bereits Unfälle passiert sind.<br>\n",
    "Bei genauerem Hinsehen wird auch ersichtlich, dass die Unfälle, welche sich an Kreuzungen oder Auffahrten ereignen eine höhere Letalität haben, also die Unfälle, die sich auf geraden Strecken ereignen.<br>\n",
    "Zusätzlich kann festgestellt werden, dass es einige Hotspots gibt, an denen sich besonders viele Unfälle ereignen. Hierbei handelt es sich ebenfalls meist um Kreuzungen und Auffahrten.<br>\n",
    "Es können noch weitere Schlüsse aus der Karte gezogen werden, welche an dieser Stelle jedoch nicht alle aufgelistet werden.\n",
    "\n",
    "-------------------------------------------------------------\n",
    "<h4 style=\"color:red\">Eigentliches Fazit. Noch irgendwas zu schreiben? Sollte etwas länger sein.</h4>\n",
    "Nach den bereits erwähnten Startschwierigkeiten, welche sowohl dem Abstimmen zwischen Kafka Producer und Kafka Consumer, sowie dem nicht funktionierenden Multi-Processing im Jupyter Notebook (auf Windows) geschuldet waren, konnten alle Visualisierungen ohne weitere Probleme umgesetzt werden. Zur Umsetzung  der visuellen Auswertung war es notwendig, sich mit den verschiedenen Bibliotheken zur Umsetzung der Virtualisierung auseinanderzusetzen. Während der Entwicklung hat sich schnell gezeigt, dass wir nicht mit allen Daten auf einmal arbeiten können, da sonst das Testen von neuen Codeabschnitten sehr viel Zeit benötigt hätte. Aus diesem Grund kamen wir zum Entschluss, die im Abschnitt `Import benötigter Module` erläuterte `DEBUG` Variable einzufügen, damit es möglich ist, neue Codeabschnitte schneller zu testen und diese erst nach erfolgreichen Tests mit dem kompletten Datensatz auszuführen. Diese `DEBUG` Variable bietet sich auch an, wenn es darum geht, dieses Notebook zu Testen, ohne womöglich über mehrere Stunden alle Daten aus den CSV-Dateien zu importieren und zu laden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
