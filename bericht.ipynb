{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einleitung\n",
    "\n",
    "In den Straßen New York Citys herrscht ein hohes Verkehrsaufkommen, wodurch Verkehrsunfälle an der Tagesordnung sind. Durch das NYC Open Data Project (https://opendata.cityofnewyork.us/) werden die erfassten Verkehrsunfälle für die Allgemeinheit zugänglich gemacht.\n",
    "\n",
    "Thema dieses Berichtes und Thema unseres Projektes ist es, die öffentlich zugänglichen Unfalldaten zu nutzen, um darauf die im Abschnitt `Analyseziele` beschriebenen Analysen durchzuführen und die Ergebnisse zu visualisieren.\n",
    "\n",
    "## Aufbau der Data Pipeline\n",
    "\n",
    "Für das Projekt soll eine Data-Pipeline aufgebaut werden, welche die Daten in die Datenbank lädt. \n",
    "Von dort aus sollen die Daten von einem Python-Script zur Analyse angefragt werden.\n",
    "\n",
    "Beteiligt an der Data-Pipeline ist eine Kafka-Instanz, wobei ein Python-Script einen Producer dafür darstellt und ein weiteres Python-Script einen Consumer. Das Producer-Script lädt die Daten zeilenweise aus der Datenquelle, einer CSV-Datei, in Kafka ein, während der Consumer die Daten aus Kafka ausliest und in die MongoDB schreibt. Das Analyse-Script bezieht die Daten wiederum aus der MongoDB.\n",
    "\n",
    "### Installation der benötigten Docker-Container\n",
    "\n",
    "Das Script `start_docker.sh` startet den MongoDB Docker Container, beziehungsweise erstellt bei Bedarf einen neuen Docker Container. Dabei wird, falls noch kein Image für die DockerDB vorhanden ist, das neueste Heruntergeladen. \n",
    "\n",
    "Die weiteren benötigten Docker Container sind in der Docker-Compose-Konfiguration (`docker-compose.yml`) vorhanden.\n",
    "Diese können mit `docker-compose up` erstellt oder heruntergeladen werden. \n",
    "Mit `docker-compose down` können die gestarteten Container gestoppt werden, wobei auf die Vollendigung des Befehls gewartet werden sollte.\n",
    "\n",
    "Auf manchen Systemen befindet sich die Docker Engine nach stoppen der Docker Container mit `docker-compose down` in einem nicht-revertierbarem Fehlerzustand, wobei ein _Neustart des Computers_ unausweichlich ist. Weitere Informationen zu dem Fehler, wobei jedoch _keine der angegebenen Workarounds genutzt werden sollte_, findet sich [auf GitHub](https://github.com/docker/for-linux/issues/162). \n",
    "\n",
    "## Datenquelle\n",
    "\n",
    "Als Datenquellen dienen die vom NYC Open Data Project bereitgestellten Unfalldaten des NYPD. Die detaillierten Unfalldaten befinden sich in 3 unterschiedlichen Datenquellen die unter Anderem als CSV-Datei verfügbar sind: [Motor Vehicle Collisions](https://data.cityofnewyork.us/browse?Data-Collection_Data-Collection=Motor+Vehicle+Collisions&q=crashes).\n",
    "\n",
    "Enthalten in den Daten sind die am Unfall beteiligten Verkehrsteilnehmer, die Verletzten und Toten, sowie die genaue Position des Unfalls. Darüberhinaus sind noch weitere Daten in den Quellen enthalten, auf die teilweise im Analyseabschnitt genauer eingegangen wird.\n",
    "\n",
    "## Analyseziele\n",
    "\n",
    "Ziel der Analyse der Daten ist herauszufinden, welche saisonalen und lokalen Zusammenhänge festzustellen sind, wodurch sich besondere Hotspots im Stadtraum New Yorks lokalisieren lassen. Des weiteren soll ermittelt werden, welche Verkehrsteilnehmer besonders oft getötet oder verletzt werden, sowie an welchen Stellen sich gerade schwerwiegende Unfälle häufen. Anhand einer animierten Karte kann dadurch das Unfallgeschehen über die Zeit betrachtet werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation der Python Packages\n",
    "\n",
    "Damit nicht alles vollständig von uns entwickelt werden muss, wurden verschiedene Bibliotheken verwendet. Auf diese Weise kann sich die Umsetzung auf das Wesentliche beziehen.\n",
    "\n",
    "Apache Kafka (https://kafka.apache.org/) soll dazu genutzt werden, mit Hilfe eines Producers und eines Consumers die Datensätze zeilenweise einzulesen. Dies wird durch die kafka-python Bibliothek (https://github.com/dpkp/kafka-python) stark vereinfacht.\n",
    "\n",
    "Während der Kafka Consumer die Datensätze registiert und ausliest müssen diese ein die MongoDB eingetragen werden. Um dieses Verfahren zu vereinfachen, wird die pymongo Bibliothek (https://pypi.org/project/pymongo/) genutzt.\n",
    "\n",
    "Es folgt die Installation der Bibliotheken über `pip install`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install kafka-python pymongo Cartopy folium rasterio earthpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importieren benötigter Module\n",
    "\n",
    "Damit die verschiedenen Funktionen und Datentypen der Bibliotheken genutzt werden können, müssen die entsprechenden Module importiert werden. Im folgenden Abschnitt werden alle notwendigen Module, die im Laufe des Projektes genutzt werden importiert.\n",
    "\n",
    "Zusätzlich hat sich während der Entwicklung gezeigt, dass je nach Betriebssystem Unterschiede bezüglich der Paralellisierung von Prozessen entstehen. Aus diesem Grund wird hier eine Variable `windows` deklariert, mit der im Laufe des zwischen Windows und anderen Betriebssystemen unterschieden werden kann.\n",
    "\n",
    "Zuletzt wird hierbei die Breite des Jupyter Notebooks auf 100% gestellt, um die Lesbarkeit zu verbessern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "from pymongo import MongoClient\n",
    "import datetime as dt\n",
    "import requests\n",
    "import os\n",
    "from pathlib import Path\n",
    "from multiprocessing import Process\n",
    "from time import sleep\n",
    "from data.schemas import schemas\n",
    "import platform\n",
    "windows = True if 'Windows' in platform.system() else False \n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\n",
    "    \"<style>.container { width:100% !important; }</style>\"\n",
    "))\n",
    "\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Herunterladen der Datensätze\n",
    "Zum Herunterladen der Datensätze werden die Daten von der Datenquelle mithilfe des nachfolgenden Python-Scripts heruntergeladen. Resultat sind drei `.csv`-Dateien, welche die Unfalldaten zeilenweise enthalten.\n",
    "Die Dateien werden im Ordner `data` gespeichert, damit sie nur einmalig heruntergeladen werden müssen. Der Ordner befindet sich im gleichen Verzeichnis wie dieses Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data/'):\n",
    "    os.mkdir('data')\n",
    "\n",
    "for file_name, download_url in [\n",
    "    ('crashes.csv', 'https://data.cityofnewyork.us/api/'\n",
    "     'views/h9gi-nx95/rows.csv?accessType=DOWNLOAD'),\n",
    "    ('vehicles.csv', 'https://data.cityofnewyork.us/api/'\n",
    "     'views/bm4k-52h4/rows.csv?accessType=DOWNLOAD'),\n",
    "    ('persons.csv', 'https://data.cityofnewyork.us/api/'\n",
    "     'views/f55k-p6yu/rows.csv?accessType=DOWNLOAD'),\n",
    "]:\n",
    "    if not os.path.isfile(fp:= (Path('data') / file_name)):\n",
    "        with open(fp, 'wb') as crash_file:\n",
    "            crash_file.write(requests.get(download_url).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einlesen der Daten und senden über den Producer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produzieren der CSV-Daten in Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import send_to_kafka\n",
    "                \n",
    "producers = {\n",
    "    'crashes': Process(\n",
    "        target=send_to_kafka, args=[\n",
    "            'crashes', 5000 if DEBUG else None\n",
    "    ]),\n",
    "    'vehicles': Process(\n",
    "        target=send_to_kafka, args=[\n",
    "            'vehicles', 5000 if DEBUG else None\n",
    "    ]),\n",
    "    'persons': Process(\n",
    "        target=send_to_kafka, args=[\n",
    "            'persons', 5000 if DEBUG else None\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konsumieren der Daten und Import in die Datenbank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst werden die Daten aus den Topics `nyc_persons` und `nyc_vehicles` verarbeitet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import process_topic\n",
    "\n",
    "consumers = {\n",
    "    'vehicles': Process(\n",
    "        target=process_topic, args=(\n",
    "        'vehicles', schemas['vehicles']\n",
    "    )),\n",
    "    'persons': Process(\n",
    "        target=process_topic, args=(\n",
    "        'persons', schemas['persons']\n",
    "    )),\n",
    "    'crashes': Process(\n",
    "        target=process_topic, args=(\n",
    "        'crashes', schemas['crashes']\n",
    "    )),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starten der Prozesse\n",
    "Nun werden die Prozesse zum Produzieren der Daten aus `vehicles.csv`, `persons.csv`, und `crashes.csv` und die Prozesse zum Konsumieren der Topics `nyc_vehicles` und `nyc_persons` gestartet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if windows:\n",
    "    limit = send_to_kafka('vehicles', 5000 if DEBUG else None)\n",
    "    process_topic('vehicles', schemas['vehicles'], limit)\n",
    "    limit = send_to_kafka('persons', 5000 if DEBUG else None)\n",
    "    process_topic('persons', schemas['persons'], limit)\n",
    "    limit = send_to_kafka('crashes', 5000 if DEBUG else None)\n",
    "    process_topic('crashes', schemas['crashes'], limit)\n",
    "else:\n",
    "    for topic in 'persons vehicles crashes'.split(' '):\n",
    "        producers[topic].start()\n",
    "    sleep(2)\n",
    "    for topic in 'persons vehicles'.split(' '):\n",
    "        consumers[topic].start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starten des Join-Prozesses (nicht auf Windows)\n",
    "Im letzten Schritt muss der Prozess gestartet werden, der das joinen der beiden Tabellen `vehicles` und `persons` in die Daten des Topics `nyc_crashes` durchführt.\n",
    "\n",
    "Wichtig ist hierbei, dass die parallelisierten Prozesse zum Produzieren aller Daten und zum Einlesen der `nyc_vehicles` und `nyc_persons` Topics beendet sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not windows:\n",
    "    assert False, 'breakpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not windows:\n",
    "    consumers['crashes'].start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datenanalyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ergebnis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (data_analytics)",
   "language": "python",
   "name": "pycharm-caae33a3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
