{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einleitung\n",
    "\n",
    "In den Straßen New York Citys herrscht ein hohes Verkehrsaufkommen, wodurch Verkehrsunfälle an der Tagesordnung sind. Durch [NYC Open Data](https://opendata.cityofnewyork.us/) werden die erfassten Verkehrsunfälle für die Allgemeinheit zugänglich gemacht.\n",
    "\n",
    "Thema dieses Berichtes und Thema unseres Projektes ist es, die öffentlich zugänglichen Unfalldaten zu nutzen, um darauf die im Abschnitt `Analyseziele` beschriebenen Analysen durchzuführen und die Ergebnisse zu visualisieren.\n",
    "\n",
    "## Ausführungsplan\n",
    "\n",
    "Bevor der eigentliche Bericht und der entsprechende Code genauer behandelt werden, wird hier erläutert, was installiert und beachtet werden muss, um einen möglichst reibungslosen Ablauf zu garantieren.\n",
    "\n",
    "### Installation der benötigten Docker-Container\n",
    "\n",
    "Damit keine weiteren Komponenten manuell installiert werden müssen und keine Änderungen am ausführenden System entstehen, werden alle Komponenten durch [Docker](https://docs.docker.com/get-docker/) installiert und ausgeführt. Somit wird Docker benötigt und muss installiert werden.\n",
    "\n",
    "Die benötigten Docker-Container sind in der Docker-Compose-Konfiguration (`docker-compose.yml`) definiert.\n",
    "Diese können mit `docker-compose up -d` heruntergeladen und gestartet werden. \n",
    "Mit `docker-compose down` können die gestarteten Container gestoppt werden. Das Terminal sollte nicht geschlossen werden, bevor alle Container durch `docker-compose down` gestoppt vollständig gestoppt wurden.\n",
    "\n",
    "Durch Nutzung einer angepassten `docker-compose.yml` konnten Fehler, die mit der Verwendung einer vorgefertigten `docker-compose.yml` entstanden sind, ausgeräumt werden.\n",
    "\n",
    "### Starten des Programmcodes\n",
    "\n",
    "Nachdem alle Container über `docker-compose up -d` gestartet wurden, kann mit der Ausführung des Jupyter Notebooks begonnen werden. Hierbei kann unter `Kernel` die Option `Restart & Run All` genutzt werden, um das komplette Notebook auszuführen. Sollte der Code nicht auf einem Windows-System ausgeführt werden, so befindet sich im Kapitel `Starten des Join-Prozesses (außer auf Windows)` eine Art Breakpoint. Dieser Breakpoint dient (wie unten beschrieben) dazu, dass der Join-Prozess nicht ausgeführt wird, bevor alle notwendigen Daten geladen sind. Zur Ausführung des Codes unter dem Breakpoint sind die Anweisungen am Anfang des entsprechenden Kapitels zu befolgen.\n",
    "\n",
    "## Aufbau der Data Pipeline\n",
    "\n",
    "Für das Projekt soll eine Data-Pipeline aufgebaut werden, welche die Daten in die Datenbank lädt. \n",
    "Von dort aus sollen die Daten von einem Python-Script zur Analyse angefragt werden.\n",
    "\n",
    "Beteiligt an der Data-Pipeline ist eine Kafka-Instanz, wobei ein Python-Script einen Producer dafür darstellt und ein weiteres Python-Script einen Consumer. Das Producer-Script lädt die Daten zeilenweise aus der Datenquelle, einer CSV-Datei, in Kafka ein, während der Consumer die Daten aus Kafka ausliest und in die MongoDB schreibt. Das Analyse-Script bezieht die Daten wiederum aus der MongoDB.\n",
    "\n",
    "## Datenquellen\n",
    "\n",
    "Als Datenquellen dienen die vom NYC Open Data Project bereitgestellten Unfalldaten des NYPD. Die detaillierten Unfalldaten befinden sich in drei unterschiedlichen Datenquellen die unter Anderem als CSV-Datei verfügbar sind: [Motor Vehicle Collisions](https://data.cityofnewyork.us/browse?Data-Collection_Data-Collection=Motor+Vehicle+Collisions&q=crashes).\n",
    "\n",
    "Enthalten in den Daten sind die am Unfall beteiligten Verkehrsteilnehmer, die Verletzten und Toten, sowie die genaue Position des Unfalls. Darüberhinaus sind noch weitere Daten in den Quellen enthalten, auf die teilweise im Analyseabschnitt genauer eingegangen wird.\n",
    "\n",
    "## Analyseziele\n",
    "\n",
    "Nach der Verarbeitung und Aufbereitung der Daten soll aus der Analyse hervorgehen, welche saisonalen und lokalen Zusammenhänge aus den Unfalldaten gelesen werden können. Des Weiteren soll das Ergebnis eine Aussage darüber geben, ob sich bestimmte Unfallhotspots innerhalb von New York lokalisieren lassen und ob diese Hotspots sich mit der Zeit ändern. Während der Ansicht der Unfallhotspots soll es auch möglich sein, zu erkennen, an welchem Hotspot sich besonders viele schwerwiegende Unfälle ereignen (Unfälle mit vielen verletzten oder getöteten Verkehrsteilnehmern). Zuletzt soll aus der Analyse hervorgehen, welche Verkehrsteilnehmer besonders oft getötet und verletzt werden. Hierzu zählen sowohl die Füßgänger, die Fahrradfahrer als auch die Fahrer von motorisierten Fahrzeugen.\n",
    "\n",
    "Die Visualisierung der Daten erfolgt über verschiedene Diagramme sowie eine Heatmap, welche mehrere Filtermöglichkeiten bereitstellt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation der Python Packages\n",
    "\n",
    "Damit nicht alles vollständig von uns entwickelt werden muss, wurden verschiedene Bibliotheken verwendet. Auf diese Weise kann sich die Umsetzung auf das Wesentliche beziehen.\n",
    "\n",
    "[Apache Kafka](https://kafka.apache.org/) soll dazu genutzt werden, mit Hilfe eines Producers und eines Consumers die Datensätze zeilenweise einzulesen. Dies wird durch die [kafka-python](https://github.com/dpkp/kafka-python) Bibliothek stark vereinfacht.\n",
    "\n",
    "Während der Kafka Consumer die Datensätze registiert und ausliest müssen diese ein die MongoDB eingetragen werden. Um dieses Verfahren zu vereinfachen, wird die [pymongo](https://pypi.org/project/pymongo/) Bibliothek genutzt.\n",
    "\n",
    "Nachdem alle Daten in der Datenbank sind, folgt die Auswertung der Daten. Um aus den Daten eine visuelle Darstellung in Form einer Heatmap aufbauen zu können, kommt die [folium](https://github.com/python-visualization/folium) Bibliothek zum Einsatz.\n",
    "\n",
    "Es folgt die Installation der genannten Bibliotheken über `pip install`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install kafka-python pymongo folium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importieren benötigter Module\n",
    "\n",
    "Damit die verschiedenen Funktionen und Datentypen der Bibliotheken genutzt werden können, müssen die entsprechenden Module importiert werden. Im folgenden Abschnitt werden alle notwendigen Module, die im Laufe des Projektes genutzt werden importiert.\n",
    "\n",
    "Zusätzlich hat sich während der Entwicklung gezeigt, dass je nach Betriebssystem Unterschiede bezüglich der Paralellisierung von Prozessen entstehen. Aus diesem Grund wird hier eine Variable `windows` deklariert, mit der während der Ausführung bestimmter Funktionen zwischen Windows und anderen Betriebssystemen unterschieden werden kann.\n",
    "\n",
    "Bei der `DEBUG` Variable handelt es sich um eine Variable, welche beim Wert `TRUE` dafür sorgt, dass nur ein Bruchteil der Daten aus den CSV-Dateien importiert wird. Der Import aller Daten (`DEBUG=FALSE`) kann mehrere Stunden in Anspruch nehmen, da es über 9 Millionen Zeilen sind (Stand 12.11.2020). `Mit DEBUG=TRUE` Variable dauert der Import der Daten nur wenige Minuten.\n",
    "\n",
    "Zuletzt wird hierbei die Breite des Jupyter Notebooks auf 100% gestellt, um die Lesbarkeit zu verbessern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "from pymongo import MongoClient\n",
    "import datetime as dt\n",
    "import requests\n",
    "import os\n",
    "from pathlib import Path\n",
    "from multiprocessing import Process\n",
    "from time import sleep\n",
    "from data.schemas import schemas\n",
    "import platform\n",
    "import folium\n",
    "windows = True if 'Windows' in platform.system() else False \n",
    "DEBUG = True\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\n",
    "    \"<style>.container { width:100% !important; }</style>\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verarbeitung der Daten\n",
    "\n",
    "Um mit der Verarbeitung der Daten zu beginnen, müssen die Daten zunächst aufbereitet und in die MongoDB eingefügt werden. Damit mit Hilfe der MongoDB verschiedene Analysen durchgeführt werden können, sind verschiedene Schritte notwendig, welche in diesem Kapitel nacheinander erläutert und durchgeführt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Herunterladen der Datensätze\n",
    "\n",
    "Da die Daten nur verarbeitet werden können, während sie diesem Notebook zur Verfügung stehen, müssen die Daten im ersten Schritt heruntergeladen werden. Hierzu werden die Daten von der Datenquelle mithilfe des nachfolgenden Python-Scripts heruntergeladen. Resultat sind drei `.csv`-Dateien, welche die Unfalldaten zeilenweise enthalten.\n",
    "Die Dateien werden im Ordner `data` gespeichert, damit sie nur einmalig heruntergeladen werden müssen. Der Ordner befindet sich im gleichen Verzeichnis wie dieses Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data/'):\n",
    "    os.mkdir('data')\n",
    "\n",
    "for file_name, download_url in [\n",
    "    ('crashes.csv', 'https://data.cityofnewyork.us/api/'\n",
    "     'views/h9gi-nx95/rows.csv?accessType=DOWNLOAD'),\n",
    "    ('vehicles.csv', 'https://data.cityofnewyork.us/api/'\n",
    "     'views/bm4k-52h4/rows.csv?accessType=DOWNLOAD'),\n",
    "    ('persons.csv', 'https://data.cityofnewyork.us/api/'\n",
    "     'views/f55k-p6yu/rows.csv?accessType=DOWNLOAD'),\n",
    "]:\n",
    "    if not os.path.isfile(fp:= (Path('data') / file_name)):\n",
    "        with open(fp, 'wb') as crash_file:\n",
    "            crash_file.write(requests.get(download_url).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produzieren der CSV-Daten in Kafka\n",
    "\n",
    "Nachdem die Daten heruntergeladen wurden, beginnt die eigentliche Verarbeitung.\n",
    "Im folgenden Codeabschnitt wird hierfür die Funktion `send_to_kafka` genutzt, die in das Modul `pipeline_tools.py` ausgelagert wurde. \n",
    "Die Funktion erstellt zunächst eine Instanz des `KafkaProducer`, entsprechend des angegebenen Topics.\n",
    "Um die Daten zu produzieren muss zeilenweise über die entsprechende `.csv` Datei iteriert werden, wobei jedoch ein Laden der gesamten Datei in den RAM vermieden wird, da es sich um einen Generator handelt.\n",
    "Nachdem eine Zeile ausgelesen wurde, wird diese in das entsprechende `topic` gesandt und muss von einem `KafkaConsumer` wieder ausgelesen werden, um die Daten nutzen zu können.\n",
    "\n",
    "Für jedes der 3 Topics wird ein eigener Producer Process definiert.\n",
    "Die einzelnen Prozesse können auf UNIX-basierten Systemen parallel ausgeführt werden, wodurch die Ausführung durch Nutzung von mehr logischen Prozessoren beschleunigt wird. \n",
    "Hierbei werden gezielt Prozesse gestartet und keine Threads, da Multithreading in Python bei Ausführung nicht-atomarer Funktionen mit Race Conditions verbunden ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline_tools import send_to_kafka\n",
    "                \n",
    "producers = {\n",
    "    'crashes': Process(\n",
    "        target=send_to_kafka, args=[\n",
    "            'crashes', 15000 if DEBUG else None\n",
    "    ]),\n",
    "    'vehicles': Process(\n",
    "        target=send_to_kafka, args=[\n",
    "            'vehicles', 5000 if DEBUG else None\n",
    "    ]),\n",
    "    'persons': Process(\n",
    "        target=send_to_kafka, args=[\n",
    "            'persons', 5000 if DEBUG else None\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Konsumieren der produzierten Daten und Import in die Datenbank\n",
    "\n",
    "Im nächsten Schritt wird die Funktion aufgerufen, mit der die Daten aus Kafka ausgelesen und in die MongoDB eingefügt werden. Diese Funktion befindet sich ebenfalls im Modul `pipeline_tools.py`.\n",
    "\n",
    "In der Funktion wird zunächst eine Verbindung zur MongoDB aufgebaut.\n",
    "Sobald die Verbindung zur Datenbank bereit ist, wird ein `KafkaConsumer` erstellt, der das übergebene `topic` abonniert. \n",
    "Durch das Abonnieren eines Topics erhält der Consumer zeilenweise die zuvor in das Topic gelesenen Daten. \n",
    "Jede Zeile wird nach dem Einlesen zunächst in eine Liste konvertiert, indem ein CSV-Reader darauf angewandt wird. \n",
    "Da es sich um Daten im CSV-Format handelt, die Strings mit KOmmas enthalten, würde ein Aufruf von `split(',')` fehlerhafte Ergebnisse liefern.\n",
    "\n",
    "Durch das Trennen in die einzelnen Bestandteile, sind die Daten nun bereit als Key-Value Paare in das entsprechende Dokument innerhalb der MongoDB persistiert zu werden.\n",
    "Dabei wird das mitgegebene Schema verwendet um die Datentypen bei Bedarf zu konvertieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline_tools import process_topic\n",
    "\n",
    "consumers = {\n",
    "    'vehicles': Process(\n",
    "        target=process_topic, args=(\n",
    "        'vehicles', schemas['vehicles']\n",
    "    )),\n",
    "    'persons': Process(\n",
    "        target=process_topic, args=(\n",
    "        'persons', schemas['persons']\n",
    "    )),\n",
    "    'crashes': Process(\n",
    "        target=process_topic, args=(\n",
    "        'crashes', schemas['crashes']\n",
    "    )),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starten der Prozesse\n",
    "\n",
    "Nachdem die Funktionen, die für das Speichern der CSV-Daten in die Datenbank zuständig sind, nun fertig definiert sind, werden diese hier aufgerufen. Da die Daten zu Personen und Fahrzeugen (Topic `nyc_persons` und `nyc_vehicles`) im Anschluss an das Persistieren in der Datenbank mit den Unfällen (`crashes`) verbunden werden müssen, werden sie vollständig verarbeitet, bevor die Daten zu den Unfällen in die Datenbank geschrieben werden. \n",
    "Dies wurde für das Windows Betriebssystem und andere Betriebssysteme unterschiedlich umgesetzt:\n",
    "\n",
    "Bei der Ausführung der Funktionen wird zwischen Microsoft Windows und anderen Betriebssystemen unterschieden:\n",
    "- Windows:<br>\n",
    "    Da es nicht möglich ist mehrere Child Processes im Jupyter Notebook zu starten, müssen die Funktionsaufrufe nacheinander und blockierend erfolgen. Durch die Synchronität können direkt alle Funktionen aufgerufen werden.\n",
    "- UNIX:<br>\n",
    "    Hier werden die Producer für die Topics `nyc_crashes`, `nyc_vehicles`, `nyc_persons` parallel gestartet. Außerdem werden die Consumer der Topics `nyc_vehicles` und `nyc_persons` kurz darauf gestartet. Sobald die beiden Consumer-Prozesse fertig sind, was daran erkenntlich wird, wenn sie nicht mehr in die Datenbank schreiben, können die nachfolgenden Prozesse manuell gestartet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline_tools import aggregate_data\n",
    "\n",
    "if windows:\n",
    "    limit = send_to_kafka('vehicles', 5000 if DEBUG else None)\n",
    "    process_topic('vehicles', schemas['vehicles'], limit)\n",
    "    limit = send_to_kafka('persons', 5000 if DEBUG else None)\n",
    "    process_topic('persons', schemas['persons'], limit)\n",
    "    limit = send_to_kafka('crashes', 15000 if DEBUG else None)\n",
    "    process_topic('crashes', schemas['crashes'], limit)\n",
    "    \n",
    "    aggregate_data()\n",
    "else:\n",
    "    for topic in 'persons vehicles crashes'.split(' '):\n",
    "        producers[topic].start()\n",
    "    sleep(2)\n",
    "    for topic in 'persons vehicles'.split(' '):\n",
    "        consumers[topic].start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starten des Join-Prozesses (UNIX)\n",
    "Im letzten Schritt muss der Prozess gestartet werden, der das Verbinden der beiden Tabellen `vehicles` und `persons` in die Daten des Topics `nyc_crashes` durchführt. Dieser Schritt wurde auf Windows bereits im vorherigen Codeabschnitt erledigt, da dieser auf Windows nicht paralellisiert wird. Das heißt, dass die Vollständigkeit der Daten bereits durch Synchronität gewährleistet wird.\n",
    "\n",
    "Für UNIX-Systeme ist wichtig, dass die parallelisierten Prozesse zum Produzieren aller Daten und zum Einlesen der `nyc_vehicles` und `nyc_persons` Topics beendet sind. Die Ausführung des vorherigen Codeblocks sollte also abgeschlossen sein. Damit die folgende Funktion bei einem `Kernel` → `Restart & Run All` nicht zu vor Beendigung der geforkten Prozesse ausgeführt wird, wurden 2 Breakpoints eingefügt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not windows:\n",
    "    assert False, 'non-Windows breakpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausführung erst nach Beendigung der drei Producer\n",
    "if not windows:\n",
    "    consumers['crashes'].start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not windows:\n",
    "    assert False, 'non-Windows breakpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausführung erst nach Beendigung des `nyc_crashes` Consumers\n",
    "if not windows:\n",
    "    aggregate_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datenanalyse\n",
    "\n",
    "Da an dieser Stelle alle Daten vollständig in die Datenbank importiert und anschließend so aufbereitet wurden, dass sie für eine visuelle Analyse verwendet werden können, beginnt an dieser Stelle die Datenanalyse. Wie im Kapitel `Analyseziele` bereits erwähnt, werden zur visuellen Darstellung Diagramme und eine Heatmap genutzt, welche vor der Erstellung nochmal detailliert erläutert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 25, 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "with MongoClient('mongodb://localhost:27017/') as client:\n",
    "    database = client[\"nyc_crashes\"]\n",
    "    crashes = database.crashes\n",
    "    crashes_by_year = database.crashes_by_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yearly(Year):\n",
    "    data = crashes_by_year.find_one({\"_id\": Year})['by_month']\n",
    "    \n",
    "    x_values = data.keys()\n",
    "    y_values_injured = []\n",
    "    y_values_killed = []\n",
    "    for month in data:\n",
    "        injured = 0\n",
    "        killed = 0\n",
    "        injured = data[month]['pedestrians']['injured'] + data[month]['cyclists']['injured'] + data[month]['motorists']['injured']\n",
    "        killed = data[month]['pedestrians']['killed'] + data[month]['cyclists']['killed'] + data[month]['motorists']['killed']\n",
    "        y_values_injured += [injured]\n",
    "        y_values_killed += [killed]\n",
    "    \n",
    "    show_graph_injury(x_values, y_values_injured, y_values_killed)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yearly_sum(Year):\n",
    "    data = crashes_by_year.find_one({\"_id\": Year})['by_month']\n",
    "    \n",
    "    x_values = data.keys()\n",
    "    y_values_pedestrians = []\n",
    "    y_values_cyclists = []\n",
    "    y_values_motorists = []\n",
    "    for month in data:\n",
    "        pedestrians = 0\n",
    "        cyclists = 0\n",
    "        motorists = 0\n",
    "        pedestrians = data[month]['pedestrians']['sum']\n",
    "        cyclists = data[month]['cyclists']['sum']\n",
    "        motorists = data[month]['motorists']['sum']\n",
    "        y_values_pedestrians += [pedestrians]\n",
    "        y_values_cyclists += [cyclists]\n",
    "        y_values_motorists += [motorists]\n",
    "    \n",
    "    show_graph_type(x_values, y_values_pedestrians, y_values_cyclists, y_values_motorists)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_graph_injury(x_values, y_values_injured, y_values_killed):\n",
    "    plt.figure(figsize=(20,10))\n",
    "\n",
    "    plt.bar(x_values, y_values_killed, color=(0.62,0.07,0.04))\n",
    "    plt.bar(x_values, y_values_injured, bottom=y_values_killed, color=(0.80,0.52,0.00))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_graph_type(x_values, y_values_pedestrians, y_values_cyclists, y_values_motorists):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    \n",
    "    p1 = plt.bar(x_values, y_values_motorists, color=(0.80,0.50,0.20))\n",
    "    p2 = plt.bar(x_values, y_values_cyclists, bottom=y_values_motorists, color=(0.75,0.75,0.75))\n",
    "    bottom_gold = [a+b for a,b in zip(y_values_cyclists, y_values_motorists)]\n",
    "    p3 = plt.bar(x_values, y_values_pedestrians, bottom=bottom_gold, color=(1.00,0.84,0.00))\n",
    "    \n",
    "    plt.ylabel('Anzahl Unfaelle')\n",
    "    plt.title('Unfaelle pro Monat nach Art')\n",
    "    plt.legend((p3[0], p2[0], p1[0]), ('Pedestrians', 'Cyclists', 'Motorists'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(yearly, Year=[data['_id'] for data in crashes_by_year.find().sort('_id', -1)]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(yearly_sum, Year=[data['_id'] for data in crashes_by_year.find().sort('_id', -1)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaktive Unfallkarte\n",
    "\n",
    "Mit der Unfallkarte ist es möglich, die Daten nach verschiedenen Unfallkriterien zu untersuchen. Beispiel hierfür sind zum Beispiel die verletzten Fahrradfahrer oder die getöteten Fußgänger.\n",
    "\n",
    "Die Unfallkarte wird mit Hile der [folium](https://github.com/python-visualization/folium) Bibliothek erstellt und benötigt zur Erstellung lediglich die benötigten Daten und ein wenig Konfiguration.\n",
    "\n",
    "<h4 style=\"color:red\">Könnte dann nochmal mehr geschrieben werden, wenn das Kartenupdate mit filtern da ist</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis_tools import crash_map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ergebnis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
